{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkgVLFgCANE5Dd+6lmyGPg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RevazRevazashvili/DataScraping/blob/main/ncbiotech_directory_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloudscraper pandas beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJui-M8TYkAg",
        "outputId": "09228254-29dc-4c84-b5b3-4b0f9562d562"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cloudscraper\n",
            "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.12/dist-packages (from cloudscraper) (3.2.3)\n",
            "Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from cloudscraper) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from cloudscraper) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.9.2->cloudscraper) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.9.2->cloudscraper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.9.2->cloudscraper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.9.2->cloudscraper) (2025.8.3)\n",
            "Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cloudscraper\n",
            "Successfully installed cloudscraper-1.2.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import threading\n",
        "\n",
        "# Thread-local storage for scrapers to avoid conflicts\n",
        "thread_local = threading.local()\n",
        "\n",
        "\n",
        "def get_scraper():\n",
        "    \"\"\"Get a thread-local scraper instance\"\"\"\n",
        "    if not hasattr(thread_local, 'scraper'):\n",
        "        thread_local.scraper = cloudscraper.create_scraper()\n",
        "    return thread_local.scraper\n",
        "\n",
        "\n",
        "def get_urls(url):\n",
        "    s = get_scraper()\n",
        "    r = s.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    urls_amount = soup.find('div',\n",
        "                            class_='views-element-container block block-views block-views-blockcompany-directory-search-company-directory-search-block').find(\n",
        "        'header').find('p').find('span').text.split()[-1]\n",
        "    pagination = int(urls_amount) // 50 + 1\n",
        "    urls = []\n",
        "\n",
        "    print(f\"Found {urls_amount} companies, fetching URLs across {pagination} pages...\")\n",
        "\n",
        "    for i in tqdm(range(pagination), desc=\"Fetching company URLs\", unit=\"page\"):\n",
        "        r = s.get(f\"{url}&page={i}\")\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "        raw_urls = [a.find('a') for a in soup.find_all('td', class_='views-field views-field-title')]\n",
        "        urls.extend([\"https://directory.ncbiotech.org\" + a.get('href') for a in raw_urls])\n",
        "\n",
        "        # Small delay to be respectful to the server\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    return urls\n",
        "\n",
        "\n",
        "def scrape_single_url(url):\n",
        "    \"\"\"Scrape a single company URL - designed for parallel execution\"\"\"\n",
        "    try:\n",
        "        s = get_scraper()\n",
        "        res = s.get(url)\n",
        "        inner_soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "        # Basic company info\n",
        "        name = inner_soup.find('div', class_='block block-core block-page-title-block').get_text(strip=True)\n",
        "        desc_tag = inner_soup.find('div', id='company-body')\n",
        "        desc = desc_tag.get_text(strip=True) if desc_tag else \"\"\n",
        "        web_tag = inner_soup.find('div',\n",
        "                                  class_='field field--name-field-company-website field--type-link field--label-hidden field__item')\n",
        "        web = web_tag.find('a').get('href') if web_tag else \"\"\n",
        "\n",
        "        # Addresses\n",
        "        wrap = inner_soup.find(\"div\", class_=\"company-mailing-address-wrap\")\n",
        "        first_address = wrap.find_all('div', class_=lambda s: s and \"mailing\" in s.split('-')) if wrap else []\n",
        "        address = \", \".join([div.get_text(strip=True) for div in first_address])\n",
        "        second_address = wrap.find_all('div', class_=lambda s: s and \"alternate\" in s.split('-')) if wrap else []\n",
        "        alternate_address = \", \".join([div.get_text(strip=True) for div in second_address])\n",
        "\n",
        "        # Phone\n",
        "        phone_tag = inner_soup.find('div', class_='company-phone location')\n",
        "        phone = phone_tag.find('div', class_='field__item').get_text(strip=True) if phone_tag else \"\"\n",
        "        pho = {\"Phone\": phone}\n",
        "\n",
        "        # Country & Region\n",
        "        country_tag = inner_soup.find('div', class_='company-country location')\n",
        "        country = country_tag.find('div', class_='field__item').get_text(strip=True) if country_tag else \"\"\n",
        "        region_tag = inner_soup.find('div', class_='company-region location')\n",
        "        region = region_tag.find('div', class_='field__item').get_text(strip=True) if region_tag else \"\"\n",
        "\n",
        "        # --- Company Details Parsing ---\n",
        "        details = {}\n",
        "        for cell in inner_soup.select(\".cell.small-12.medium-6, .cell.small-12.medium-12\"):\n",
        "            label = cell.find(\"div\", class_=\"field__label\")\n",
        "            if not label:\n",
        "                continue\n",
        "            label_text = label.get_text(strip=True)\n",
        "\n",
        "            values = []\n",
        "            for v in cell.find_all([\"div\"], class_=[\"field__item\", \"field_item\"]):\n",
        "                values.append(v.get_text(strip=True))\n",
        "\n",
        "            if len(values) == 1:\n",
        "                details[label_text] = values[0]\n",
        "            elif len(values) > 1:\n",
        "                details[label_text] = values\n",
        "\n",
        "        record = {\n",
        "            \"Name\": name,\n",
        "            \"Description\": desc,\n",
        "            \"Website\": web,\n",
        "            \"Address\": address,\n",
        "            \"Alternate Address\": alternate_address,\n",
        "            \"Phone\": phone,\n",
        "            \"Country\": country,\n",
        "            \"Region\": region\n",
        "        }\n",
        "\n",
        "        # Merge company details into main record\n",
        "        record.update(details)\n",
        "        record.update(pho)\n",
        "\n",
        "        return record\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def scrape_urls_parallel(urls, max_workers=10):\n",
        "    \"\"\"Scrape URLs in parallel with progress bar\"\"\"\n",
        "    records = []\n",
        "    failed_urls = []\n",
        "\n",
        "    print(f\"\\nScraping {len(urls)} company pages with {max_workers} parallel workers...\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all tasks\n",
        "        future_to_url = {executor.submit(scrape_single_url, url): url for url in urls}\n",
        "\n",
        "        # Process completed tasks with progress bar\n",
        "        for future in tqdm(as_completed(future_to_url), total=len(urls), desc=\"Scraping companies\", unit=\"company\"):\n",
        "            url = future_to_url[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    records.append(result)\n",
        "                else:\n",
        "                    failed_urls.append(url)\n",
        "            except Exception as e:\n",
        "                print(f\"Exception for {url}: {str(e)}\")\n",
        "                failed_urls.append(url)\n",
        "\n",
        "    if failed_urls:\n",
        "        print(f\"\\nWarning: Failed to scrape {len(failed_urls)} URLs\")\n",
        "        print(\"Failed URLs:\")\n",
        "        for url in failed_urls[:5]:  # Show first 5 failed URLs\n",
        "            print(f\"  - {url}\")\n",
        "        if len(failed_urls) > 5:\n",
        "            print(f\"  ... and {len(failed_urls) - 5} more\")\n",
        "\n",
        "    return records\n",
        "\n",
        "\n",
        "def save_as_csv(data, filename=\"companies.csv\"):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\nSaved {len(data)} company records to {filename}\")\n",
        "\n",
        "\n",
        "def find_main_urls(url):\n",
        "    s = get_scraper()\n",
        "    r = s.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    type_urls = soup.find(\n",
        "      \"div\",\n",
        "      class_=\"field field--name-field-home-searches-heading field--type-string field--label-hidden field__item\",\n",
        "      string=lambda text: text and \"Type\" in text\n",
        "    )\n",
        "    main_urls = [\"https://directory.ncbiotech.org\"+a.get('href') for a in type_urls.find_next_sibling().find_all('a')]\n",
        "    return main_urls\n",
        "\n",
        "\n",
        "def main(max_workers=10):\n",
        "    \"\"\"Main function with configurable parallelism\"\"\"\n",
        "    print(\"Starting NC Biotech Directory Scraper...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Get main category URLs\n",
        "    print(\"Fetching main category URLs...\")\n",
        "    main_urls = find_main_urls('https://directory.ncbiotech.org/')\n",
        "    print(f\"Found {len(main_urls)} main categories\")\n",
        "\n",
        "    # Collect all business URLs\n",
        "    all_business_urls = []\n",
        "    for i, url in enumerate(main_urls, 1):\n",
        "        print(f\"\\nProcessing category {i}/{len(main_urls)}: {url}\")\n",
        "        business_urls = get_urls(url)\n",
        "        all_business_urls.extend(business_urls)\n",
        "        print(f\"Found {len(business_urls)} companies in this category\")\n",
        "\n",
        "    print(f\"\\nTotal companies to scrape: {len(all_business_urls)}\")\n",
        "\n",
        "    # Scrape all URLs in parallel\n",
        "    start_time = time.time()\n",
        "    data = scrape_urls_parallel(all_business_urls, max_workers=max_workers)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Save results\n",
        "    save_as_csv(data)\n",
        "\n",
        "    print(f\"\\nScraping completed in {end_time - start_time:.2f} seconds\")\n",
        "    print(f\"Successfully scraped {len(data)} out of {len(all_business_urls)} companies\")\n",
        "    print(f\"Success rate: {len(data) / len(all_business_urls) * 100:.1f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can adjust max_workers based on your needs and server tolerance\n",
        "    # Start with 10, increase if the server can handle it, decrease if you get errors\n",
        "    main(max_workers=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5e9f9mSeGGU",
        "outputId": "645aaf72-a2aa-4fc5-853f-7948d5667ac0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting NC Biotech Directory Scraper...\n",
            "==================================================\n",
            "Fetching main category URLs...\n",
            "Found 2 main categories\n",
            "\n",
            "Processing category 1/2: https://directory.ncbiotech.org/company-directory?f[0]=search_by_company_type:10226\n",
            "Found 912 companies, fetching URLs across 19 pages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching company URLs: 100%|██████████| 19/19 [00:03<00:00,  4.86page/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 912 companies in this category\n",
            "\n",
            "Processing category 2/2: https://directory.ncbiotech.org/company-directory?f[0]=search_by_company_type:10231\n",
            "Found 2407 companies, fetching URLs across 49 pages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching company URLs: 100%|██████████| 49/49 [00:10<00:00,  4.50page/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2307 companies in this category\n",
            "\n",
            "Total companies to scrape: 3219\n",
            "\n",
            "Scraping 3219 company pages with 10 parallel workers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping companies: 100%|██████████| 3219/3219 [03:55<00:00, 13.69company/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved 3219 company records to companies.csv\n",
            "\n",
            "Scraping completed in 235.71 seconds\n",
            "Successfully scraped 3219 out of 3219 companies\n",
            "Success rate: 100.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NiO3-URnC_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}